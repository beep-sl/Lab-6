{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6\n",
        "# Stephen and Zahra\n",
        "# 3/30/23"
      ],
      "metadata": {
        "id": "0upmaCGf8rTk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1uNktQ5fkYZ"
      },
      "source": [
        "Download pre-trained Word2Vec model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mfg3M1occ0tD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7919242-ccb8-4f65-8c4f-5fad4327abbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 98.5% 31.2/31.6MB downloaded"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "corpus = api.load('text8')\n",
        "model = Word2Vec(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3RciG-646lif"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDDHVMNtfpTn"
      },
      "source": [
        "Getting embedding of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "evXtyPGDfsXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13dd6851-49e6-4855-a5b1-84330bcc8b90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.7832766 , -0.96575844,  0.47220847, -0.3736778 , -0.02325084,\n",
              "       -0.7958983 ,  0.4800855 , -0.68430334,  0.01864947,  0.25354746,\n",
              "        0.4675374 , -0.9785528 , -1.2125181 , -0.6186081 , -0.5388376 ,\n",
              "        1.8834221 ,  1.7704828 , -1.0579275 ,  1.2803311 , -0.7503278 ,\n",
              "       -1.9556098 , -0.7494621 ,  0.30723435, -0.5918319 , -0.2440902 ,\n",
              "        0.6957924 , -1.1008555 , -0.8925937 , -0.52666163, -0.6116081 ,\n",
              "        0.3110509 ,  0.19051307, -0.24196912, -0.22351381,  0.14732872,\n",
              "        1.7896966 ,  0.39720222, -0.67234474, -0.12932627,  1.011306  ,\n",
              "       -0.48131087, -1.2953782 , -0.52418375,  0.43842018, -0.27650204,\n",
              "       -0.92612255, -0.5635319 , -0.2362394 , -1.7477231 , -0.7285474 ,\n",
              "        0.74319345,  0.27074164, -0.6248669 , -0.6571851 , -1.0153799 ,\n",
              "       -1.1947662 ,  0.2783715 , -0.05961734,  0.9347939 ,  0.06033426,\n",
              "        0.43768364, -0.6868789 ,  0.91989005,  1.1135072 ,  0.42156565,\n",
              "       -0.6620511 ,  0.7229523 ,  0.8793481 ,  0.08634901,  0.49075767,\n",
              "       -1.0783281 , -0.55442506, -0.12985629, -0.39618877,  0.3542701 ,\n",
              "       -0.05236741,  0.59953815, -0.6614758 , -0.5208353 , -0.09311286,\n",
              "        0.7257003 ,  0.29310417, -0.3271793 ,  0.56878227,  0.5899966 ,\n",
              "        0.85928816, -0.2115047 ,  2.3999448 , -0.07478236,  0.33691484,\n",
              "       -0.8563931 ,  0.63793427,  0.11089189, -1.1532549 ,  1.0742073 ,\n",
              "       -0.12203337,  0.29125792,  1.0277432 ,  0.5064672 ,  0.35529467],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_sentence_embedding(model, text):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = text.split(\" \")\n",
        "  count = 0\n",
        "  for i in range(1, len(words)):\n",
        "    try:\n",
        "      if count == 0:\n",
        "        vector = model.wv[words[i]]\n",
        "      else: \n",
        "        vector = np.copy(vector+model.wv[words[i]])\n",
        "      count+=1\n",
        "    except:\n",
        "      continue\n",
        "  return vector/count\n",
        "\n",
        "# Sample code to extract vector for a sentence\n",
        "get_sentence_embedding(model, \"test text embedding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhkT9ECgqM_"
      },
      "source": [
        "Reading TSV file and saving embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u30ytolQgt7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af784336-3d14-42b6-b6cb-3584fffa8b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\t\t number of instances: 3005\n",
            "positive\t\t number of instances: 2263\n",
            "negative\t\t number of instances: 8997\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_tweets_get_vectors(tweet_file_path):\n",
        "  # This method takes in the file path for the twitter file, and return a\n",
        "  # dicationary of dictionaries. In the first dictionary the keys are the\n",
        "  # tweet labels (3 classes), and the values are another dictionary with\n",
        "  # tweet id as the key and values are tuple of (vector, tweet text)\n",
        "    df = pd.read_csv(tweet_file_path, sep=',', header=0)\n",
        "    dic_result = {}\n",
        "    df1 = df[['tweet_id', 'text', 'airline_sentiment']]\n",
        "    for index in range(len(df1)):\n",
        "        try:\n",
        "            vetor_rep = get_sentence_embedding(model, df.loc[index, \"text\"].lower())\n",
        "            label = df.loc[index, \"airline_sentiment\"]\n",
        "            tweet_id = df.loc[index, \"tweet_id\"]\n",
        "            if label in dic_result:\n",
        "                dic_result[label][tweet_id] = (vetor_rep, df.loc[index, \"text\"].lower())\n",
        "            else:\n",
        "                dic_result[label] = {tweet_id: (vetor_rep, df.loc[index, \"text\"].lower())}\n",
        "        except:\n",
        "            pass\n",
        "    return dic_result\n",
        "\n",
        "twitter_data = read_tweets_get_vectors(\"Tweets.csv\")\n",
        "for key in twitter_data.keys():\n",
        "  print(key + \"\\t\\t number of instances: \" + str(len(twitter_data[key])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkN4pKvwRlVp"
      },
      "source": [
        "Code to generate training, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5TcXsEVYuYab"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_data(twitter_data):\n",
        "  # takes in the dictionary from the previous step and generate\n",
        "  # the training, validation, and test sets. Note that the labels \n",
        "  # are represented as one-hot codings.\n",
        "    training_x = []\n",
        "    training_y = []\n",
        "\n",
        "    validation_x = []\n",
        "    validation_y = []\n",
        "\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "\n",
        "    for label in twitter_data:\n",
        "\n",
        "        # labels are indicated as one hot coding [negative, neutral, positive]\n",
        "        if label == \"negative\":\n",
        "            n_label = [1, 0, 0]\n",
        "        elif label == \"neutral\":\n",
        "            n_label = [0, 1, 0]\n",
        "        else:\n",
        "            n_label = [0, 0, 1]\n",
        "        temp_dic = twitter_data[label]\n",
        "        lst_tweet_ids = list(temp_dic.keys())\n",
        "        #### Splitting by 80-10-10\n",
        "        ## Note that you could alternatively use sklearn split method\n",
        "        train_length = int(len(lst_tweet_ids)*0.8)\n",
        "        train_ids = lst_tweet_ids[ :train_length]\n",
        "        remaining = lst_tweet_ids[train_length:]\n",
        "        test_lenght = int(len(remaining)*0.5)\n",
        "        test_ids = remaining[:test_lenght]\n",
        "        validation_id = remaining[test_lenght:]\n",
        "\n",
        "        for tweet_id in train_ids:\n",
        "            training_x.append(temp_dic[tweet_id][0])\n",
        "            training_y.append(n_label)\n",
        "        for tweet_id in validation_id:\n",
        "            validation_x.append(temp_dic[tweet_id][0])\n",
        "            validation_y.append(n_label)\n",
        "        for tweet_id in test_ids:\n",
        "            test_x.append(temp_dic[tweet_id][0])\n",
        "            test_y.append(n_label)\n",
        "\n",
        "    # The reason we apply this shuffling is to make sure \n",
        "    # when passing batches to the network, we see different items \n",
        "    c = list(zip(training_x, training_y))\n",
        "    random.shuffle(c)\n",
        "    training_x, training_y = zip(*c)\n",
        "\n",
        "    return training_x, training_y, validation_x, validation_y, test_x, test_y\n",
        "\n",
        "# Sample usage\n",
        "training_x, training_y, validation_x, validation_y, test_x, test_y = split_data(twitter_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kEhk44VbwtQ"
      },
      "source": [
        "Here goes your code for your Feedfoward network Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6WEyopGmd5fn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Define layers\n",
        "\n",
        "        self.layer_1 = nn.Linear(input_dim, hidden_dim_1)\n",
        "\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        self.layer_2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
        "\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        self.layer_3 = nn.Linear(hidden_dim_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your network forward pass\n",
        "\n",
        "        # modify this line\n",
        "\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu_1(out)\n",
        "\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        out = self.layer_3(out)\n",
        "\n",
        "        return torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val_losses(train_losses, val_losses):\n",
        "    # Plot the training and validation losses\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(val_losses, label='Validation')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tH1QbZQ66QFp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojspmxozek-V"
      },
      "source": [
        "Training the network\n",
        "Define a class, with properties such as size of hidden layers\n",
        "loss function, optimizer, training method, test method, and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "unZLRP-7etiL"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# here goes your parameters\n",
        "# sample code to define your model \n",
        "#model = FeedforwardNeuralNetModel(input_dim, hidden_dim_1, out_dim)\n",
        "#model.to(devide)\n",
        "input_dim = 100\n",
        "hidden_dim_1 = 16\n",
        "hidden_dim_2 = 32\n",
        "hidden_dim_3 = 32\n",
        "output_dim = 3\n",
        "num_epochs = 10000\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim_1, hidden_dim_2, \n",
        "                                  hidden_dim_3, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1e-3)\n",
        "model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  # this method will be used to calculate the accuracy of your model\n",
        "    correct = (y_true.argmax (dim = 1) == y_pred.argmax (dim = 1)).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "def training(tfidfX_train, Y_train, tfidfX_val, Y_val):\n",
        "  # this method will be used for training your model\n",
        "  # inputs are the training and validation sets\n",
        "  # You can define batch size of your choice\n",
        "  batch_size = 2000\n",
        "  #print(type(tfidfX_train))\n",
        "  X_train_mini_batches = torch.split(tfidfX_train, batch_size)\n",
        "  Y_train_mini_batches = torch.split(Y_train, batch_size)\n",
        "  train_losses = []\n",
        "  train_accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "  best_accuracy = 0\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      epoch_loss = 0\n",
        "      epoch_accuracy = 0\n",
        "      validation_loss = 0\n",
        "      val_accuracy = 0\n",
        "      for X_train_mini_batch, Y_train_mini_batch in zip(X_train_mini_batches, Y_train_mini_batches):\n",
        "          X_train_mini_batch = X_train_mini_batch.to(device)\n",
        "          Y_train_mini_batch = Y_train_mini_batch.to(device)\n",
        "          # Continue code here to train the network\n",
        "          # here check your validation set\n",
        "          # you have to save the model with the best loss or maybe accuracy?\n",
        "          train_prediction = model.forward(X_train_mini_batch.float())\n",
        "          train_prediction = torch.squeeze(train_prediction)\n",
        "          train_loss = criterion(train_prediction, Y_train_mini_batch.float())\n",
        "          optimizer.zero_grad()\n",
        "          train_loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += train_loss.item()\n",
        "          epoch_accuracy += calculate_accuracy(Y_train_mini_batch, train_prediction)\n",
        "\n",
        "      tfidfX_val = tfidfX_val.to(device)\n",
        "      Y_val = Y_val.to(device)\n",
        "      val_prediction = model.forward(tfidfX_val.float())\n",
        "      val_prediction = torch.squeeze(val_prediction)\n",
        "      val_loss = criterion(val_prediction, Y_val.float())\n",
        "      validation_loss = val_loss.item()\n",
        "      val_accuracy = calculate_accuracy(Y_val, val_prediction)\n",
        "      if val_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), \"best_model_state.bin\")\n",
        "        best_accuracy = val_accuracy\n",
        "      epoch_loss /= len(X_train_mini_batches)\n",
        "      epoch_accuracy /= len(X_train_mini_batches)\n",
        "      val_losses.append(validation_loss)\n",
        "      train_losses.append(epoch_loss)\n",
        "      train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "      #plot_train_val_losses(train_losses, val_losses)\n",
        "\n",
        "      val_accuracies.append(val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IemtXv1hSZaD",
        "outputId": "11e0a709-0972-414d-e0b3-b2a7646bda1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11411, 100])\n",
            "torch.Size([11411, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [02:46<00:00, 60.05it/s]\n"
          ]
        }
      ],
      "source": [
        "tfidfX_train = torch.tensor(training_x)\n",
        "Y_train = torch.tensor(training_y)\n",
        "tfidfX_val = torch.tensor(validation_x)\n",
        "Y_val = torch.tensor(validation_y)\n",
        "\n",
        "print(tfidfX_train.shape)\n",
        "print(Y_train.shape)\n",
        "training(tfidfX_train, Y_train, tfidfX_val, Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YhDaRwalHnxk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def test(tfidfX_test, Y_test):\n",
        "  tfidfX_test = tfidfX_test.to(device)\n",
        "  Y_test = Y_test.to(device)\n",
        "\n",
        "  #print(tfidfX_test.shape)\n",
        "  #print(Y_test.shape)\n",
        "\n",
        "  test_prediction = model.forward(tfidfX_test.float())\n",
        "  test_prediction = torch.squeeze(test_prediction)\n",
        "\n",
        "  test_accuracy = calculate_accuracy(Y_test, test_prediction)\n",
        "\n",
        "  print(\"Test accuracy:\", round(test_accuracy.item(),4), \"\\n\")\n",
        "\n",
        "  #test_prediction = test_prediction.to(device)\n",
        "  #test_prediction = test_prediction.ge(.5).view(-1).cpu()\n",
        "  Y_test = Y_test.cpu()\n",
        "\n",
        "  a = test_prediction.argmax(dim=1).numpy()\n",
        "  b = Y_test.argmax(dim=1).numpy()\n",
        "\n",
        "  print(classification_report(b, a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek322sSCOeqm",
        "outputId": "f64d7cb1-5dd8-46d7-8591-b753bd5edc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.6311 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      1.00      0.77       900\n",
            "           1       0.00      0.00      0.00       300\n",
            "           2       0.00      0.00      0.00       226\n",
            "\n",
            "    accuracy                           0.63      1426\n",
            "   macro avg       0.21      0.33      0.26      1426\n",
            "weighted avg       0.40      0.63      0.49      1426\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "tfidfX_test = torch.tensor(test_x)\n",
        "Y_test = torch.tensor(test_y)\n",
        "\n",
        "test(tfidfX_test, Y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}